# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BrQQlI8yYL2HgCHgjJSd7_B20hT0zo3Y
"""

!pip install requests pandas

# Step 1: Set Up Authentication
token = "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"  # Replace with your actual GitHub token
username = "tanmay8542"  # Replace with your GitHub username
email = "kerlikartanmay854@gmail.com"  # Replace with your email

# Configure Git in Colab
!git config --global user.name "{tanmay8542}"
!git config --global user.email "{kerlikartanmay854@gmail.com}"

!git clone https://{"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"}@github.com/{"tanmay8542"}/{"project1"}.git

# Commented out IPython magic to ensure Python compatibility.
 #Step 3: Copy Your Colab File to the Repository Folder
!cp /content/Untitled8.ipynb /content/project1/

# Step 4: Navigate to the Repository Folder
# %cd /content/project1

# Step 5: Add, Commit, and Push Changes
!git add .
!git commit -m "Add Colab notebook"
!git push origin main

import requests
import pandas as pd
import time
import logging
from typing import List, Dict, Any

class GitHubScraper:
    def __init__(self, token: str):
        """
        Initialize the GitHub scraper with your API token.

        Args:
            token (str): GitHub Personal Access Token
        """
        self.headers = {
            'Authorization': f'token {token}',
            'Accept': 'application/vnd.github.v3+json'
        }
        self.base_url = 'https://api.github.com'

        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def _make_request(self, url: str, params: dict = None) -> Dict:
        """
        Make a request to the GitHub API with rate limit handling.
        """
        while True:
            response = requests.get(url, headers=self.headers, params=params)

            if response.status_code == 200:
                return response.json()
            elif response.status_code == 403:
                reset_time = int(response.headers.get('X-RateLimit-Reset', 0))
                sleep_time = max(reset_time - time.time(), 0) + 1
                self.logger.warning(f"Rate limit hit. Sleeping for {sleep_time} seconds")
                time.sleep(sleep_time)
            else:
                self.logger.error(f"Error {response.status_code}: {response.text}")
                response.raise_for_status()

    def clean_company_name(self, company: str) -> str:
        """
        Clean up company names according to specifications.
        """
        if not company:
            return ""

        # Strip whitespace and @ symbol
        cleaned = company.strip().lstrip('@')

        # Convert to uppercase
        return cleaned.upper()

    def search_users(self, location: str, min_followers: int) -> List[Dict]:
        """
        Search for GitHub users in a specific location with minimum followers.
        """
        users = []
        page = 1

        while True:
            self.logger.info(f"Fetching users page {page}")

            query = f"location:{location} followers:>={min_followers}"
            params = {
                'q': query,
                'per_page': 100,
                'page': page
            }

            url = f"{self.base_url}/search/users"
            response = self._make_request(url, params)

            if not response['items']:
                break

            for user in response['items']:
                user_data = self._make_request(user['url'])

                # Extract only the required fields with exact matching names
                cleaned_data = {
                    'login': user_data['login'],
                    'name': user_data['name'] if user_data['name'] else "",
                    'company': self.clean_company_name(user_data.get('company')),
                    'location': user_data['location'] if user_data['location'] else "",
                    'email': user_data['email'] if user_data['email'] else "",
                    'hireable': user_data['hireable'] if user_data['hireable'] is not None else False,
                    'bio': user_data['bio'] if user_data['bio'] else "",
                    'public_repos': user_data['public_repos'],
                    'followers': user_data['followers'],
                    'following': user_data['following'],
                    'created_at': user_data['created_at']
                }

                users.append(cleaned_data)

            page += 1

        return users

    def get_user_repositories(self, username: str, max_repos: int = 500) -> List[Dict]:
        """
        Get repositories for a specific user.
        """
        repos = []
        page = 1

        while len(repos) < max_repos:
            self.logger.info(f"Fetching repositories for {username}, page {page}")

            params = {
                'sort': 'pushed',
                'direction': 'desc',
                'per_page': 100,
                'page': page
            }

            url = f"{self.base_url}/users/{username}/repos"
            response = self._make_request(url, params)

            if not response:
                break

            for repo in response:
                # Extract only the required fields with exact matching names
                repo_data = {
                    'login': username,  # Adding owner's login as required
                    'full_name': repo['full_name'],
                    'created_at': repo['created_at'],
                    'stargazers_count': repo['stargazers_count'],
                    'watchers_count': repo['watchers_count'],
                    'language': repo['language'] if repo['language'] else "",
                    'has_projects': repo['has_projects'],
                    'has_wiki': repo['has_wiki'],
                    'license_name': repo['license']['key'] if repo.get('license') else ""
                }

                repos.append(repo_data)

            if len(response) < 100:
                break

            page += 1

        return repos[:max_repos]

def main():
    # Get GitHub token
    token = input("Enter your GitHub token: ").strip()
    if not token:
        print("Token is required. Exiting...")
        return

    # Initialize scraper
    scraper = GitHubScraper(token)

    # Search for users in Sydney with >100 followers
    users = scraper.search_users(location='Sydney', min_followers=100)

    # Save users to CSV
    users_df = pd.DataFrame(users)
    users_df.to_csv('users.csv', index=False)

    # Get repositories for each user
    all_repos = []
    for user in users:
        repos = scraper.get_user_repositories(user['login'])
        all_repos.extend(repos)

    # Save repositories to CSV
    repos_df = pd.DataFrame(all_repos)
    repos_df.to_csv('repositories.csv', index=False)

    print(f"Scraped {len(users)} users and {len(all_repos)} repositories")

    # Create README.md
    with open('README.md', 'w') as f:
        f.write(f"""# GitHub Users in Sydney

This repository contains data about GitHub users in Sydney with over 100 followers and their repositories.

## Files

1. `users.csv`: Contains information about {len(users)} GitHub users in Sydney with over 100 followers
2. `repositories.csv`: Contains information about {len(all_repos)} public repositories from these users
3. `gitscrap.py`: Python script used to collect this data

## Data Collection

- Data collected using GitHub API
- Date of collection: {time.strftime('%Y-%m-%d')}
- Only included users with 100+ followers
- Up to 500 most recently pushed repositories per user
""")

if __name__ == "__main__":
    main()

from google.colab import files
files.download('users.csv')
files.download('repositories.csv')

import pandas as pd

# Load the users data
users_df = pd.read_csv('users.csv')

# Filter for users located in Sydney
sydney_users = users_df[users_df['location'].str.contains('Sydney', case=False, na=False)]

# Sort users by followers count in descending order
top_sydney_users = sydney_users.sort_values(by='followers', ascending=False).head(5)

# Extract the login names
top_logins = top_sydney_users['login'].tolist()

# Create a comma-separated string
top_logins_string = ', '.join(top_logins)

print("Top 5 users in Sydney with the highest number of followers:")
print(top_logins_string)

import pandas as pd

# Load the users data
users_df = pd.read_csv('users.csv')

# Convert the 'created_at' column to datetime
users_df['created_at'] = pd.to_datetime(users_df['created_at'])

# Filter for users located in Sydney
sydney_users = users_df[users_df['location'].str.contains('Sydney', case=False, na=False)]

# Sort users by 'created_at' in ascending order
earliest_sydney_users = sydney_users.sort_values(by='created_at').head(5)

# Extract the login names
earliest_logins = earliest_sydney_users['login'].tolist()

# Create a comma-separated string
earliest_logins_string = ', '.join(earliest_logins)

print("Top 5 earliest registered users in Sydney:")
print(earliest_logins_string)

import csv

# Define the list to store users from Sydney
users_in_sydney = []

# Read the CSV file with UTF-8 encoding
with open('users.csv', 'r', encoding='utf-8') as file:
    reader = csv.DictReader(file)
    for row in reader:
        location = row['location'].strip().lower()
        # Check if the user is from Sydney
        if 'sydney' in location:
            users_in_sydney.append({
                'login': row['login'],
                'followers': int(row['followers'])
            })

# Sort users based on followers in descending order
top_users = sorted(users_in_sydney, key=lambda x: x['followers'], reverse=True)

# Extract the top 5 user logins
top_5_logins = [user['login'] for user in top_users[:5]]

# Print the result as a comma-separated list
print(', '.join(top_5_logins))

import csv
from collections import Counter

# Define a Counter to count license occurrences
license_counter = Counter()

# Read the CSV file with UTF-8 encoding
with open('users.csv', 'r', encoding='utf-8') as file:
    reader = csv.DictReader(file)
    for row in reader:
        license_name = row['license_name'].strip()
        # Ignore missing licenses
        if license_name:
            license_counter[license_name] += 1

# Get the 3 most common licenses
top_3_licenses = license_counter.most_common(3)

# Extract license names and format them as a comma-separated string
top_3_license_names = [license[0] for license in top_3_licenses]

# Print the result
print(', '.join(top_3_license_names))

sydney_users = users_df[users_df['location'].str.contains('Sydney', case=False, na=False)]
top_5_sydney_users = sydney_users.sort_values(by='followers', ascending=False).head(5)

# Extract the 'login' column as a comma-separated string
top_5_logins = ', '.join(top_5_sydney_users['login'])
top_5_logins

import csv
from collections import Counter

# Step 1: Create a mapping of user logins to their locations
user_locations = {}
with open('users.csv', 'r', encoding='utf-8') as user_file:
    user_reader = csv.DictReader(user_file)
    for row in user_reader:
        user_locations[row['login']] = row['location'].strip().lower()

# Step 2: Define a list to store programming languages for Sydney users
languages_sydney = []

# Step 3: Read the repositories CSV file and filter for Sydney users
with open('repositories.csv', 'r', encoding='utf-8') as repo_file:
    repo_reader = csv.DictReader(repo_file)
    for row in repo_reader:
        user_login = row['login']
        language = row.get('language', '').strip()

        # Check if the user is from Sydney and ignore empty languages
        if user_login in user_locations and 'sydney' in user_locations[user_login]:
            if language:
                languages_sydney.append(language)

# Step 4: Count the occurrence of each language
language_counts = Counter(languages_sydney)

# Step 5: Find the most common language
most_common_language = language_counts.most_common(1)

# Step 6: Print the result
if most_common_language:
    print(most_common_language[0][0])
else:
    print("No language data found for users in Sydney.")

import csv
from collections import defaultdict
from datetime import datetime

# Step 1: Create a dictionary to count repositories per user
repository_counts = defaultdict(int)

# Step 2: Read the repositories CSV file
with open('repositories.csv', 'r', encoding='utf-8') as repo_file:
    repo_reader = csv.DictReader(repo_file)
    for row in repo_reader:
        created_at = row['created_at']
        user_login = row['login']

        # Step 3: Convert created_at to a datetime object
        created_date = datetime.fromisoformat(created_at[:-1])  # Remove the 'Z' and convert

        # Step 4: Check if the day is Saturday (5) or Sunday (6)
        if created_date.weekday() in [5, 6]:  # 5 for Saturday, 6 for Sunday
            repository_counts[user_login] += 1

# Step 5: Sort the users by the number of repositories created (descending)
top_users = sorted(repository_counts.items(), key=lambda x: x[1], reverse=True)

# Step 6: Extract the top 5 user logins
top_5_logins = [user[0] for user in top_users[:5]]

# Step 7: Print the result as a comma-separated list
print(','.join(top_5_logins))
print(','.join(top_5_logins).strip())

import csv

# Define the list to store users from Sydney
users_in_sydney = []

# Read the CSV file with UTF-8 encoding
with open('users.csv', 'r', encoding='utf-8') as file:
    reader = csv.DictReader(file)
    for row in reader:
        location = row['location'].strip().lower()
        # Check if the user is from Sydney
        if 'sydney' in location:
            users_in_sydney.append({
                'login': row['login'],
                'followers': int(row['followers'])
            })

# Sort users based on followers in descending order
top_users = sorted(users_in_sydney, key=lambda x: x['followers'], reverse=True)

# Extract the top 5 user logins
top_5_logins = [user['login'] for user in top_users[:5]]

# Print the result as a comma-separated list
print(','.join(top_5_logins))

import csv

# Define the list to store users from Sydney
users_in_sydney = []

# Read the CSV file with UTF-8 encoding
with open('users.csv', 'r', encoding='utf-8') as file:
    reader = csv.DictReader(file)
    for row in reader:
        location = row['location'].strip().lower()
        # Check if the user is from Sydney
        if 'sydney' in location:
            users_in_sydney.append({
                'login': row['login'],
                'created_at': row['created_at']
            })

# Sort users based on created_at in ascending order
earliest_users = sorted(users_in_sydney, key=lambda x: x['created_at'])

# Extract the top 5 user logins
top_5_earliest_logins = [user['login'] for user in earliest_users[:5]]

# Print the result as a comma-separated list
print(','.join(top_5_earliest_logins))

import csv
from collections import Counter

# Define the list to store licenses
licenses = []

# Read the CSV file with UTF-8 encoding
with open('repositories.csv', 'r', encoding='utf-8') as file:
    reader = csv.DictReader(file)
    for row in reader:
        # Get the license name (ignore empty values)
        license_name = row.get('license_name', '').strip()
        if license_name:  # Only consider non-empty license names
            licenses.append(license_name)

# Count the occurrence of each license
license_counts = Counter(licenses)

# Get the 3 most common licenses
most_common_licenses = license_counts.most_common(3)

# Extract the license names
top_3_licenses = [license[0] for license in most_common_licenses]

# Print the result as a comma-separated list
print(','.join(top_3_licenses))

import csv
from collections import Counter

# Define the list to store companies
companies = []

# Read the CSV file with UTF-8 encoding
with open('users.csv', 'r', encoding='utf-8') as file:
    reader = csv.DictReader(file)
    for row in reader:
        # Get the company name (ignore empty values)
        company_name = row.get('company', '').strip()
        if company_name:  # Only consider non-empty company names
            companies.append(company_name)

# Count the occurrence of each company
company_counts = Counter(companies)

# Find the most common company
most_common_company = company_counts.most_common(1)

# Print the result
if most_common_company:
    print(most_common_company[0][0])  # Company name
else:
    print("No company data found.")

import csv
from collections import Counter

# Define the list to store programming languages
languages = []

# Read the CSV file with UTF-8 encoding
with open('repositories.csv', 'r', encoding='utf-8') as file:
    reader = csv.DictReader(file)
    for row in reader:
        # Get and clean up the language field (ignore empty values)
        language = row.get('language', '').strip()
        if language:  # Only consider non-empty language names
            languages.append(language)

# Count the occurrence of each language
language_counts = Counter(languages)

# Find the most common language
most_common_language = language_counts.most_common(1)

# Print the result
if most_common_language:
    print(most_common_language[0][0])  # Most popular programming language
else:
    print("No language data found.")

import csv
from collections import Counter
from datetime import datetime

# Define the list to store programming languages for users who joined after 2020
languages_after_2020 = []

# Read the users CSV file to get the creation dates
with open('users.csv', 'r', encoding='utf-8') as users_file:
    users_reader = csv.DictReader(users_file)
    users_after_2020 = []

    for user in users_reader:
        # Check if the user joined after 2020
        created_at = datetime.strptime(user['created_at'], '%Y-%m-%dT%H:%M:%SZ')
        if created_at.year > 2020:
            users_after_2020.append(user['login'])

# Read the repositories CSV file to count languages for these users
with open('repositories.csv', 'r', encoding='utf-8') as repos_file:
    repos_reader = csv.DictReader(repos_file)

    for row in repos_reader:
        if row['login'] in users_after_2020:
            language = row.get('language', '').strip()
            if language:  # Only consider non-empty language names
                languages_after_2020.append(language)

# Count the occurrence of each language
language_counts = Counter(languages_after_2020)

# Find the two most common languages
most_common_languages = language_counts.most_common(2)

# Print the second most common language if it exists
if len(most_common_languages) > 1:
    print(most_common_languages[1][0])  # Second most popular programming language
else:
    print("Less than two languages found.")

import csv
from collections import defaultdict

# Dictionary to store total stars and counts for each language
language_stars = defaultdict(lambda: {'total_stars': 0, 'repo_count': 0})

# Read the repositories CSV file
with open('repositories.csv', 'r', encoding='utf-8') as repos_file:
    repos_reader = csv.DictReader(repos_file)

    for row in repos_reader:
        language = row.get('language', '').strip()
        stars = int(row.get('stargazers_count', 0))

        if language:  # Only consider non-empty language names
            language_stars[language]['total_stars'] += stars
            language_stars[language]['repo_count'] += 1

# Calculate average stars for each language
language_averages = {}

for language, data in language_stars.items():
    if data['repo_count'] > 0:  # Avoid division by zero
        average_stars = data['total_stars'] / data['repo_count']
        language_averages[language] = average_stars

# Find the language with the highest average stars
if language_averages:
    highest_average_language = max(language_averages, key=language_averages.get)
    highest_average_value = language_averages[highest_average_language]

    print(f"The language with the highest average number of stars per repository is '{highest_average_language}' with an average of {highest_average_value:.2f} stars.")
else:
    print("No repositories found.")

import csv

# List to store users and their leader strength
leader_strengths = []

# Read the users CSV file
with open('users.csv', 'r', encoding='utf-8') as users_file:
    reader = csv.DictReader(users_file)

    for row in reader:
        login = row['login']
        followers = int(row['followers'])
        following = int(row['following'])

        # Calculate leader strength
        leader_strength = followers / (1 + following) if following >= 0 else followers

        leader_strengths.append({'login': login, 'leader_strength': leader_strength})

# Sort users by leader strength in descending order
sorted_users = sorted(leader_strengths, key=lambda x: x['leader_strength'], reverse=True)

# Get the top 5 users
top_5_users = sorted_users[:5]

# Extract logins for the output
top_5_logins = [user['login'] for user in top_5_users]

# Print the result as a comma-separated list
print(','.join(top_5_logins))

import csv
import pandas as pd

# Lists to store followers and public repositories
followers = []
public_repos = []

# Read the users CSV file
with open('users.csv', 'r', encoding='utf-8') as users_file:
    reader = csv.DictReader(users_file)

    for row in reader:
        followers.append(int(row['followers']))
        public_repos.append(int(row['public_repos']))

# Create a DataFrame to analyze the data
data = pd.DataFrame({
    'followers': followers,
    'public_repos': public_repos
})

# Calculate the correlation
correlation = data['followers'].corr(data['public_repos'])

# Print the result
print(f"Correlation between number of followers and number of public repositories: {correlation:.4f}")

import csv
import pandas as pd
import statsmodels.api as sm

# Lists to store followers and public repositories
followers = []
public_repos = []

# Read the users CSV file
with open('users.csv', 'r', encoding='utf-8') as users_file:
    reader = csv.DictReader(users_file)

    for row in reader:
        followers.append(int(row['followers']))
        public_repos.append(int(row['public_repos']))

# Create a DataFrame to analyze the data
data = pd.DataFrame({
    'followers': followers,
    'public_repos': public_repos
})

# Define the independent variable (X) and dependent variable (y)
X = data['public_repos']
y = data['followers']

# Add a constant to the independent variable
X = sm.add_constant(X)

# Fit the regression model
model = sm.OLS(y, X).fit()

# Print the regression results
print(model.summary())

# Extract the coefficient for public_repos
slope = model.params['public_repos']
print(f"Estimated additional followers per additional public repository: {slope:.4f}")

import pandas as pd

# Read the repositories CSV file
repositories = pd.read_csv('repositories.csv')

# Convert 'has_projects' and 'has_wiki' to integers (1 for True, 0 for False)
repositories['has_projects'] = repositories['has_projects'].map({'true': 1, 'false': 0})
repositories['has_wiki'] = repositories['has_wiki'].map({'true': 1, 'false': 0})

# Calculate the correlation between has_projects and has_wiki
correlation = repositories['has_projects'].corr(repositories['has_wiki'])

# Print the correlation
print(f"Correlation between having projects enabled and having wiki enabled: {correlation:.4f}")

import pandas as pd

# Read the repositories CSV file
repositories = pd.read_csv('repositories.csv')

# Check for missing values in 'has_projects' and 'has_wiki'
print("Missing values before cleaning:")
print(repositories[['has_projects', 'has_wiki']].isnull().sum())

# Clean up the has_projects and has_wiki columns
# Map 'true'/'false' to 1/0, and fill NaNs with 0 (or you can drop them)
repositories['has_projects'] = repositories['has_projects'].map({'true': 1, 'false': 0})
repositories['has_wiki'] = repositories['has_wiki'].map({'true': 1, 'false': 0})

# Fill NaN values with 0 (or you can choose to drop rows with NaN)
repositories['has_projects'] = repositories['has_projects'].fillna(0)
repositories['has_wiki'] = repositories['has_wiki'].fillna(0)

# Check again for missing values after cleaning
print("Missing values after cleaning:")
print(repositories[['has_projects', 'has_wiki']].isnull().sum())

# Calculate the correlation between has_projects and has_wiki
correlation = repositories['has_projects'].corr(repositories['has_wiki'])

# Print the correlation
print(f"Correlation between having projects enabled and having wiki enabled: {correlation:.4f}")

import pandas as pd

# Read the users CSV file
users = pd.read_csv('users.csv')

# Clean up the hireable column
# Convert 'true'/'false' to boolean and fill NaN with False
users['hireable'] = users['hireable'].map({'true': True, 'false': False}).fillna(False)

# Calculate average following count for hireable and non-hireable users
average_following = users.groupby('hireable')['following'].mean()

# Print the results
print("Average number of users followed:")
print(f"Hireable users: {average_following[True] if True in average_following.index else 'No hireable users'}")
print(f"Non-hireable users: {average_following[False] if False in average_following.index else 'No non-hireable users'}")

import pandas as pd
import numpy as np
import statsmodels.api as sm

# Load the users data from users.csv
users = pd.read_csv('users.csv')

# Filter out users without bios
users = users[users['bio'].notna()]

# Calculate the length of the bio in words
users['bio_length'] = users['bio'].apply(lambda x: len(x.split()))

# Prepare the data for regression
X = users['bio_length']  # Independent variable
y = users['followers']  # Dependent variable

# Add a constant to the independent variable
X = sm.add_constant(X)

# Fit the regression model
model = sm.OLS(y, X).fit()

# Print the summary of the regression
print(model.summary())

import csv
from collections import Counter
from datetime import datetime

# Read the CSV file with UTF-8 encoding
with open('repositories.csv', 'r', encoding='utf-8') as file:
    reader = csv.DictReader(file)
    weekend_users = []

    for row in reader:
        created_at = row['created_at']
        if created_at:  # Ensure created_at is not empty
            # Parse the created_at date
            created_date = datetime.strptime(created_at, '%Y-%m-%dT%H:%M:%SZ')
            # Check if the date is a weekend (Saturday=5, Sunday=6)
            if created_date.weekday() in [5, 6]:
                weekend_users.append(row['login'])

# Count occurrences of each user
weekend_user_counts = Counter(weekend_users)

# Get the top 5 users who created the most repositories on weekends
top_5_weekend_users = weekend_user_counts.most_common(5)

# Extract the logins
top_5_logins = [user[0] for user in top_5_weekend_users]

# Print the result as a comma-separated list
print(', '.join(top_5_logins))

import csv
from collections import Counter

# Define the list to store surnames
surnames = []

# Read the CSV file with UTF-8 encoding
with open('users.csv', 'r', encoding='utf-8') as file:
    reader = csv.DictReader(file)
    for row in reader:
        name = row['name']
        if name:  # Ensure the name is not empty
            # Split the name by whitespace and take the last word as the surname
            surname = name.strip().split()[-1]
            surnames.append(surname)

# Count the occurrences of each surname
surname_counts = Counter(surnames)

# Find the most common surname(s)
most_common_surnames = surname_counts.most_common()
max_count = most_common_surnames[0][1]  # Get the count of the most common surname
common_surnames = [surname for surname, count in most_common_surnames if count == max_count]

# Sort the surnames alphabetically
common_surnames.sort()

# Print the result as a comma-separated list
print(', '.join(common_surnames))

import csv
from collections import Counter
from datetime import datetime

# Define lists to store users and programming languages
filtered_users = []
languages = []

# Read the users.csv file to filter users who joined after 2020
with open('users.csv', 'r', encoding='utf-8') as user_file:
    user_reader = csv.DictReader(user_file)
    for row in user_reader:
        created_at = row['created_at']
        if created_at:  # Ensure the created_at is not empty
            created_date = datetime.strptime(created_at, '%Y-%m-%dT%H:%M:%SZ')
            if created_date.year > 2020:  # Check if user joined after 2020
                filtered_users.append(row['login'])  # Store the login of the user

# Read the repositories.csv file to count programming languages for the filtered users
with open('repositories.csv', 'r', encoding='utf-8') as repo_file:
    repo_reader = csv.DictReader(repo_file)
    for row in repo_reader:
        if row['login'] in filtered_users:  # Check if the repository belongs to the filtered users
            language = row.get('language', '').strip()
            if language:  # Ensure the language is not empty
                languages.append(language)

# Count the occurrences of each language
language_counts = Counter(languages)

# Find the second most common language
if len(language_counts) >= 2:
    # Sort the languages by count
    second_most_common_language = language_counts.most_common(2)[1][0]
else:
    second_most_common_language = "Not enough data"

# Print the result
print(second_most_common_language)

import csv
from collections import Counter
from datetime import datetime

# Define lists to store programming languages and filtered users
languages = []
filtered_users = []

# Read the users.csv file to filter users from Sydney who joined after 2020
with open('users.csv', 'r', encoding='utf-8') as user_file:
    user_reader = csv.DictReader(user_file)

    for row in user_reader:
        # Check if the user is from Sydney
        location = row.get('location', '').strip().lower()
        created_at = row.get('created_at', '').strip()

        # Convert the date string to a datetime object
        if created_at and 'sydney' in location:
            user_join_date = datetime.strptime(created_at, "%Y-%m-%dT%H:%M:%SZ")

            # Check if the user joined after 2020
            if user_join_date.year > 2020:
                # Store the user's login for later use
                filtered_users.append(row['login'])

# Read the repositories.csv file to count programming languages for the filtered users
with open('repositories.csv', 'r', encoding='utf-8') as repo_file:
    repo_reader = csv.DictReader(repo_file)

    for row in repo_reader:
        # Check if the repository belongs to the filtered users
        if row['login'] in filtered_users:
            # Get the language field and clean it up
            language = row.get('language', '').strip()
            if language:
                languages.append(language)

# Count the occurrence of each language
language_counts = Counter(languages)

# Find the two most common languages
most_common_languages = language_counts.most_common(2)

# Print the second most common language
if len(most_common_languages) >= 2:
    print(most_common_languages[1][0])  # Second most common language
else:
    print("Not enough language data found.")

import csv
import numpy as np
import statsmodels.api as sm

# Define lists to store bio lengths and followers
bio_lengths = []
followers_counts = []

# Read the users.csv file with UTF-8 encoding
with open('users.csv', 'r', encoding='utf-8') as file:
    reader = csv.DictReader(file)

    for row in reader:
        bio = row.get('bio', '').strip()
        followers = int(row.get('followers', 0))

        # Ignore users without a bio
        if bio:
            # Calculate the length of the bio in words
            bio_length = len(bio.split())
            bio_lengths.append(bio_length)
            followers_counts.append(followers)

# Prepare the data for regression analysis
X = np.array(bio_lengths)  # Independent variable
y = np.array(followers_counts)  # Dependent variable

# Add a constant to the independent variable (required for statsmodels)
X = sm.add_constant(X)

# Perform OLS regression
model = sm.OLS(y, X).fit()

# Print the regression results
print(model.summary())

# Print the coefficient for bio length
print(f"Coefficient for bio length: {model.params[1]}")